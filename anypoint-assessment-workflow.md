# Anypoint Platform Live Hierarchy Report - Workflow Execution Guide

**Purpose:** This workflow guide contains the step-by-step process for generating comprehensive Anypoint Platform hierarchy reports using MuleSoft MCP Server Tools and Enhanced Dynamic Discovery.

**Output:** Generates timestamped reports using the `anypoint-assessment-template.md` template

---

## ğŸ› ï¸ **WORKFLOW EXECUTION STEPS**

### Step 1: Get Platform Insights
```
Use MCP Tool: get_platform_insights
Parameters:
- includeFlows: true
- includeMessages: true  
- includeDataThroughput: true
```

**Purpose:** Retrieve comprehensive platform metrics including flow counts, message volumes, and data throughput across all organizations and environments.

### Step 1.5: Get Reuse Metrics
```
Use MCP Tool: get_reuse_metrics
Parameters:
- includeReuseMetrics: true
```

**Purpose:** Retrieve application reuse metrics to identify number of consumers for each application, enabling better understanding of application adoption and impact.

---

### Step 2: Dynamic Business Group and Environment Discovery
**APPROACH:** Dynamically discover all organizations and environments without relying on static environment names.

#### Step 2.1: Extract Organization and Environment Inventory from Platform Insights
From Step 1 results, build complete inventory:
```
Platform Insights Analysis:
- Extract ALL unique combinations: (org_id, org_name, env_id, env_name)
- Create comprehensive organization/environment matrix
- Note: Platform insights aggregates ALL environments across ALL organizations
- Build discovery list for validation in Step 2.2

Example Output:
Organization: "Acme Corp" (org-123)
  â””â”€â”€ Environments discovered: ["Production", "Sandbox", "QA"]
Organization: "Beta Ltd" (org-456)  
  â””â”€â”€ Environments discovered: ["Prod", "Dev", "Staging"]
```

#### Step 2.2: Dynamic Environment Validation Loop
**CRITICAL:** Validate each discovered environment instead of using static names:
```
For each organization discovered in Step 2.1:
  For each environment discovered for that organization:
    
    Use MCP Tool: list_applications
    Parameters:
    - environmentName: [discovered_environment_name]  # Dynamic from Step 2.1
    - organizationId: [discovered_org_id]             # Dynamic from Step 2.1
    - page: 1
    
    If successful (applications returned or empty list):
      â†’ Environment validated and accessible
      â†’ Add to validated_environments list
      â†’ Proceed to Step 3 for this environment
    
    If failed (environment not found/access denied):
      â†’ Environment inaccessible or renamed
      â†’ Log as inaccessible_environment
      â†’ Skip this environment

Example Dynamic Loop:
âœ… Validate: list_applications(environmentName="Production", organizationId="org-123")
âœ… Validate: list_applications(environmentName="Sandbox", organizationId="org-123")  
âœ… Validate: list_applications(environmentName="Prod", organizationId="org-456")
âŒ Skip: list_applications(environmentName="Old-Env", organizationId="org-789") â†’ Access Denied
```

#### Step 2.3: Environment Discovery Summary
Generate discovery summary for transparency:
```
Dynamic Discovery Results:
- Total Organizations Discovered: [X]
- Total Environments Discovered: [X]  
- Validated Accessible Environments: [X]
- Inaccessible/Renamed Environments: [X]
- Ready for Application Analysis: [X] environments across [X] organizations

Validated Environment Matrix:
Organization: "Acme Corp" (org-123)
  âœ… Production (env-abc) - [X] apps
  âœ… Sandbox (env-def) - [X] apps
  
Organization: "Beta Ltd" (org-456)  
  âœ… Prod (env-ghi) - [X] apps
  âŒ Old-Dev (env-jkl) - Access Denied
```

---

### Step 3: Dynamic Application Collection Loop
**CRITICAL**: For each validated environment from Step 2.2, retrieve ALL applications using dynamic pagination:

#### Step 3.1: Dynamic Application Collection Algorithm
```
Initialize: validated_environments = [] # From Step 2.2
Initialize: all_applications = {}

For each validated_environment in validated_environments:
  org_id = validated_environment.org_id
  org_name = validated_environment.org_name  
  env_name = validated_environment.env_name
  
  # Start pagination loop for this environment
  page = 1
  total_apps_for_env = 0
  
  While True:
    
    Use MCP Tool: list_applications
    Parameters:
    - environmentName: env_name          # Dynamic from validation
    - organizationId: org_id             # Dynamic from validation  
    - page: page                         # Dynamic pagination
    
    If response.success:
      apps_on_page = response.applications.length
      total_pages = response.total_pages
      
      # Collect application details
      For each app in response.applications:
        Record: name, status, deployment_status, v_cores, replicas, last_update_time
        Store in: all_applications[org_id][env_name][app.name]
      
      total_apps_for_env += apps_on_page
      
      # Check if more pages exist
      If page >= total_pages:
        Break  # All pages collected for this environment
      Else:
        page += 1  # Continue to next page
        
    Else:
      Log: "Failed to retrieve page {page} for {org_name}/{env_name}"
      Break  # Stop pagination for this environment

  Log: "Collected {total_apps_for_env} applications from {org_name}/{env_name}"

Final Result: Complete application inventory across all validated environments
```

#### Step 3.2: Real-World Dynamic Collection Examples
```
Example 1: Multi-Organization Discovery
âœ… Platform Insights discovers:
   - Acme Corp (org-123): ["Production", "Sandbox"]  
   - Beta Ltd (org-456): ["Prod", "Dev", "Staging"]
   - Gamma Inc (org-789): ["Live", "Test"]

âœ… Dynamic validation and collection:
   # Acme Corp - Production
   list_applications(environmentName="Production", organizationId="org-123", page=1) â†’ 15 apps, 2 pages
   list_applications(environmentName="Production", organizationId="org-123", page=2) â†’ 3 apps
   Total: 18 applications collected
   
   # Acme Corp - Sandbox  
   list_applications(environmentName="Sandbox", organizationId="org-123", page=1) â†’ 8 apps, 1 page
   Total: 8 applications collected
   
   # Beta Ltd - Prod
   list_applications(environmentName="Prod", organizationId="org-456", page=1) â†’ 25 apps, 3 pages
   list_applications(environmentName="Prod", organizationId="org-456", page=2) â†’ 10 apps  
   list_applications(environmentName="Prod", organizationId="org-456", page=3) â†’ 2 apps
   Total: 37 applications collected
   
   # Continue for all validated environments...

Example 2: Handling Access Denied Scenarios  
âŒ Some environments may be inaccessible:
   list_applications(environmentName="Old-Dev", organizationId="org-789", page=1) â†’ Access Denied
   â†’ Skip this environment, log as inaccessible
   â†’ Continue with remaining validated environments

Final Collection Summary:
- Total Organizations: 3
- Validated Environments: 5 (1 skipped due to access issues)  
- Total Applications: 63 applications across all accessible environments
```

---

### Step 3.5: Collect API Manager Policy Information
**PURPOSE**: For each application, determine if it's protected by API Manager and collect applied policies.

#### Step 3.5.1: API Instance Discovery and Policy Collection
```
CRITICAL DISCOVERY APPROACH: Use environment-wide API instance listing instead of app-specific search

For each environment in validated_environments:
  
  Use MCP Tool: list_api_instances
  Parameters:
  - environmentName: env_name
  - organizationId: org_id
  - includeAppliedPolicies: true        # Get applied policies
  - page: 1 (and iterate through all pages)
  
  # Build API instance mapping
  For each api_instance in response.instances:
    api_asset_id = api_instance.assetId
    applied_policies = api_instance.appliedPolicies
    
    # AGILE MATCHING: Use flexible, multi-strategy matching algorithm
    For each application in environment_applications:
      app_name = application.name.toLowerCase()
      api_asset_id = api_instance.assetId.toLowerCase()
      
      match_found = false
      match_strategy = ""
      
      # Strategy 1: Exact match
      If app_name == api_asset_id:
        match_found = true
        match_strategy = "exact"
      
      # Strategy 2: Application contains API asset ID (most common)
      If not match_found AND api_asset_id in app_name:
        match_found = true
        match_strategy = "contains"
      
      # Strategy 3: API asset ID contains application name
      If not match_found AND app_name in api_asset_id:
        match_found = true
        match_strategy = "reverse_contains"
      
      # Strategy 4: Remove common prefixes/suffixes and retry
      If not match_found:
        # Remove common prefixes (org names, env names, etc.)
        cleaned_app_name = remove_common_prefixes(app_name)
        cleaned_api_name = remove_common_prefixes(api_asset_id)
        
        If cleaned_app_name == cleaned_api_name OR 
           cleaned_api_name in cleaned_app_name OR 
           cleaned_app_name in cleaned_api_name:
          match_found = true
          match_strategy = "cleaned"
      
      # Strategy 5: Fuzzy matching using word overlap
      If not match_found:
        app_words = extract_meaningful_words(app_name)
        api_words = extract_meaningful_words(api_asset_id)
        
        # Calculate word overlap percentage
        overlap_score = calculate_word_overlap(app_words, api_words)
        
        If overlap_score >= 0.6:  # 60% word overlap threshold
          match_found = true
          match_strategy = "fuzzy"
      
      # Strategy 6: Semantic matching for known patterns
      If not match_found:
        # Check for semantic equivalents
        If semantic_match(app_name, api_asset_id):
          match_found = true
          match_strategy = "semantic"
          
          # Examples:
          # "marketing-xapi" â†” "marketplace-xapi" (marketing = marketplace)
          # "product-mgmt-papi" â†” "product-catalog-and-availability-papi"
          # "checkout-process" â†” "checkout-papi"
      
      # If match found by any strategy:
      If match_found:
        â†’ Application IS protected by API Manager
        â†’ Record: api_asset_id, applied_policies, match_strategy
        â†’ Calculate active policy count (exclude disabled policies)
        â†’ Store mapping: api_management_data[org_id][env_name][app_name]
        â†’ Log: "Matched {app_name} â†” {api_asset_id} via {match_strategy}"

# For applications with no API instance match:
For each unmatched_application:
  â†’ Application is NOT protected by API Manager
  â†’ Status = "Not API Managed"
  â†’ Still include in report with "0 API policies" notation
  â†’ Log: "No API instance found for {app_name}"

AGILE MATCHING EXAMPLES:
âœ… ana-product-sapi â†” product-sapi (contains strategy)
âœ… server-info â†” info-server-api (reverse_contains strategy) 
âœ… checkout-flow â†” checkout-papi (fuzzy/word overlap strategy)
âœ… marketing-xapi â†” marketplace-xapi (semantic strategy)
âœ… product-mgmt â†” product-catalog-and-availability-papi (fuzzy strategy)
âœ… order-api â†” orders-system-api (cleaned + fuzzy strategy)
âŒ mule-demo-mcp â†’ No semantic match â†’ "Not API Managed"

POLICY COUNT CALCULATION:
- Only count policies where disabled = false
- Include policy names in parentheses for active policies
- Format: "X (PolicyName1, PolicyName2)" or "0 (Not API Managed)"
- Log matching strategy used for transparency and debugging
```

#### Step 3.5.2: Consumer Count Integration
```
Cross-reference reuse metrics from Step 1.5 with application data:

For each application in all_applications:
  If app exists in reuse_metrics:
    â†’ Extract consumer_count from reuse metrics
    â†’ Record number of consumers/dependencies
    â†’ Include in application details
  
  If app NOT in reuse_metrics:
    â†’ Consumer count = "N/A" or "0"
    â†’ Still include in report
    â†’ Flag as "Reuse metrics unavailable"

Example Consumer Integration:
âœ… ana-product-sapi â†’ 12 consumers (high reuse)
âœ… ana-checkout-papi â†’ 3 consumers (moderate reuse)
ğŸ’¤ backend-info â†’ 0 consumers (no reuse)
```

### Step 4: Generate Report Using Template

#### Step 4.1: Load Report Template
Load the `anypoint-assessment-template.md` template file containing the report structure.

#### Step 4.2: Data Population Process
1. **Replace Executive Summary placeholders** with aggregated metrics from collected data
2. **Build deployment hierarchy tree** using organization/environment/application structure
3. **Generate deployment metrics tables** with organization overview and status distribution
4. **Calculate health metrics** and identify applications requiring attention
5. **Populate key findings** based on platform analysis
6. **Update timestamp** in header with current datetime

**ğŸš¨ CRITICAL REQUIREMENT - Application Detail Structure Enforcement:**

**MANDATORY**: Every single application in the hierarchy MUST follow this EXACT detail structure format:
```
â”œâ”€â”€â”€ [application-name] [STATUS_ICON] [STATUS_TEXT]
â”‚    â”œâ”€â”€â”€ ğŸ’¾ [Platform]: [vCores] vCore Ã— [replicas] replica/worker
â”‚    â”œâ”€â”€â”€ âš™ï¸ Runtime: [Mule Version] | ğŸ“ˆ Flows: [count] | Messages: [count] | Data: [amount] GB throughput
â”‚    â”œâ”€â”€â”€ ğŸ‘¥ Consumers: [count] | ğŸ›¡ï¸ API Policies: [count] ([Policy Names or "Not API Managed"])
â”‚    â””â”€â”€â”€ ğŸ•’ Last Updated: [Date]
```

**APPLICATION DETAIL VALIDATION RULES:**
- **NO EXCEPTIONS**: Every application must have all 4 detail lines (ğŸ’¾, ğŸ“ˆ, ğŸ‘¥, ğŸ•’)
- **NO SHORTCUTS**: Never list applications with just status - always include full details
- **DATA COMPLETENESS**: Use "N/S" for missing metrics, but structure must be complete
- **CONSISTENT FORMATTING**: Maintain exact indentation and icon structure

**PRE-SAVE VALIDATION REQUIREMENT:**
Before saving the report, verify that NO application appears as:
```
âŒ WRONG: â”œâ”€â”€â”€ application-name âœ… RUNNING
âŒ WRONG: â”œâ”€â”€â”€ application-name âœ… RUNNING (0.2 vCores)
```

All applications must appear as:
```
âœ… CORRECT: â”œâ”€â”€â”€ [application-name] [STATUS_ICON] [STATUS_TEXT]
            â”‚    â”œâ”€â”€â”€ ğŸ’¾ [Platform]: [vCores] vCore Ã— [replicas] replica/worker
            â”‚    â”œâ”€â”€â”€ âš™ï¸ Runtime: [Mule Version] | ğŸ“ˆ Flows: [count] | Messages: [count] | Data: [amount] GB throughput
            â”‚    â”œâ”€â”€â”€ ğŸ‘¥ Consumers: [count] | ğŸ›¡ï¸ API Policies: [count] ([Policy Names or "Not API Managed"])
            â”‚    â””â”€â”€â”€ ğŸ•’ Last Updated: [Date]

#### Step 4.3: Create Timestamped Report File
Generate the report filename using current datetime:
```
Format: anypoint-platform-assessment-report-YYYYMMDD-HHMMSS.md
Example: anypoint-platform-assessment-report-20260212-211500.md

Use current datetime in Europe/Paris timezone for consistency
```

#### Step 4.4: Save Final Report
1. Apply populated template data to generate complete report
2. Save to timestamped filename
3. Preserve original template file unchanged for future use

#### Step 4.5: Data Quality Validation & Application Status Assignment
**CRITICAL**: Perform comprehensive data validation before finalizing the report:

##### 4.5.1: Application Detail Completeness Check
```
For each application in all_applications:
  Validate required fields are present:
  - âœ… Application name
  - âœ… Deployment status (RUNNING, STOPPED, FAILED, etc.)
  - âœ… Platform type (CloudHub 2.0, CloudHub Legacy, Runtime Fabric)
  - âœ… Resource allocation (vCores, replicas)
  - âœ… Last update timestamp
  
  If missing critical details:
    â†’ Flag as "Incomplete Data" in report
    â†’ Add to "Applications Requiring Review" section
    â†’ Document missing fields for follow-up
```

##### 4.5.2: IDLE Application Status Assignment
```
MANDATORY RULE: Applications with message_count = 0 MUST be flagged as IDLE

For each application in platform insights message data:
  If app.mule_message_count == 0:
    â†’ Status = ğŸ’¤ IDLE (Running but no traffic during insight period)
    â†’ Include in "Resource Optimization" recommendations
    â†’ Calculate idle vCore consumption for cost analysis
  
  If app.mule_message_count > 0:
    â†’ Status = âœ… RUNNING (Active with traffic)
    â†’ Include message volume in application metrics

Example Status Assignment:
âœ… server-info: 10,366 messages â†’ RUNNING (Active)
ğŸ’¤ backend-info: 0 messages â†’ IDLE (No traffic)
ğŸ’¤ mule-demo-mcp: 0 messages â†’ IDLE (No traffic)
```

##### 4.5.3: Complete Application Documentation Check
```
CRITICAL: Ensure ALL applications from list_applications are documented in the report

Cross-reference validation:
1. **Platform Insights Applications**: From get_platform_insights (may be incomplete)
2. **Complete Application List**: From list_applications per environment (comprehensive)

Validation Process:
For each environment in validated_environments:
  list_apps = get_applications_from_list_applications(org_id, env_name)
  insights_apps = get_applications_from_platform_insights(org_id, env_name)
  
  For each app in list_apps:
    If app NOT in insights_apps:
      â†’ Missing metrics data (flows, messages, throughput)
      â†’ Status = "Metrics N/A" or "N/S" (Not Specified)
      â†’ Still include in report hierarchy
      â†’ Flag as "Limited Metrics Available"
    
    If app in insights_apps:
      â†’ Full metrics available
      â†’ Use insights data for flows, messages, throughput
      â†’ Apply IDLE status rule (message_count = 0)

Documentation Requirements:
- âœ… ALL applications from list_applications MUST appear in hierarchy
- âœ… Applications without insights data marked with "N/S" or "N/A" 
- âœ… Clear indication when metrics are unavailable
- âœ… No applications excluded from report due to missing metrics
```

##### 4.5.4: Environment Assignment Validation
```
Handle applications that cannot be assigned to proper API tier (SAPI/PAPI/EAPI):

Classification Rules:
1. **Check application name patterns**:
   - *-sapi, *-system-api â†’ ğŸ”µ System API
   - *-papi, *-process-api â†’ ğŸŸ¢ Process API  
   - *-eapi, *-xapi, *-experience-api â†’ ğŸŸ  Experience API

2. **If no clear pattern detected**:
   - â†’ Classify as âšª Other Applications
   - â†’ Still include full application details
   - â†’ Document in "Other Applications" section
   - â†’ No applications excluded due to classification uncertainty

3. **Special application handling**:
   - broker, mcp, workshop, demo applications â†’ ğŸ§ª Special Applications
   - High-resource applications (>0.2 vCore) â†’ ğŸ§ª Special Applications
   - Applications with unique purposes â†’ ğŸ§ª Special Applications

Example Classification:
âœ… ana-product-sapi â†’ ğŸ”µ System API (clear pattern)
âœ… mule-b2b-mcp â†’ ğŸ§ª Special Application (MCP pattern)
âœ… unknown-app â†’ âšª Other Application (no clear pattern)
```

##### 4.5.5: Final Validation Checklist
Before saving the report, confirm:
- [ ] All applications from list_applications appear in report hierarchy
- [ ] Applications with 0 messages marked as ğŸ’¤ IDLE
- [ ] Missing metrics marked as "N/S" or "N/A"
- [ ] Applications properly classified by tier (SAPI/PAPI/EAPI/Other/Special)
- [ ] Resource totals calculated correctly (vCores, replicas)
- [ ] Status distribution counts match actual application statuses
- [ ] No applications excluded due to missing data or unclear classification

#### Step 4.6: Automated Application Detail Structure Verification
**ğŸš¨ MANDATORY PRE-SAVE VALIDATION STEP - PREVENTS INCOMPLETE REPORTS**

**PURPOSE**: Automatically scan the generated report to ensure ZERO applications are missing detailed structure and status distribution counts are accurate.

##### 4.6.1: Pattern-Based Detail Structure Verification
```
AUTOMATED SCAN: Search the report for any application entries that match these INVALID patterns:

âŒ INVALID PATTERNS (MUST NOT EXIST):
- "â”œâ”€â”€â”€ [app-name] âœ… RUNNING\nâ”œâ”€â”€â”€" (missing details)
- "â”œâ”€â”€â”€ [app-name] âŒ STOPPED\nâ”œâ”€â”€â”€" (missing details)  
- "â”œâ”€â”€â”€ [app-name] ğŸ’¤ IDLE\nâ”œâ”€â”€â”€" (missing details)
- "â”œâ”€â”€â”€ [app-name] [STATUS]\nâ”‚" (where next line is NOT "â”‚    â”œâ”€â”€â”€ ğŸ’¾")

âœ… VALID PATTERNS (REQUIRED):
- "â”œâ”€â”€â”€ [app-name] [STATUS]\nâ”‚    â”œâ”€â”€â”€ ğŸ’¾" (starts detail structure)
- Every application must have 4 detail lines: ğŸ’¾, ğŸ“ˆ, ğŸ‘¥, ğŸ•’

VALIDATION ALGORITHM:
1. Extract all application names from list_applications data
2. For each application name:
   - Search for pattern: "â”œâ”€â”€â”€ {app_name}"
   - Verify next line contains: "â”‚    â”œâ”€â”€â”€ ğŸ’¾"
   - Count detail lines (must be exactly 4)
   - Verify all required icons present: ğŸ’¾, ğŸ“ˆ, ğŸ‘¥, ğŸ•’

If ANY application fails validation:
  â†’ STOP REPORT GENERATION
  â†’ Log missing applications with details
  â†’ FIX report before proceeding
  â†’ RE-RUN validation until 100% pass rate
```

##### 4.6.4: CRITICAL - Application Status Count Validation Against Hierarchy
```
ğŸš¨ MANDATORY FINAL VALIDATION: Count applications from ANYPOINT PLATFORM DEPLOYMENT HIERARCHY to ensure Status Distribution table accuracy

HIERARCHY-BASED COUNTING ALGORITHM:
1. Parse the "ğŸ¢ ANYPOINT PLATFORM DEPLOYMENT HIERARCHY" section
2. For each organization, count applications by status from actual hierarchy:
   
   Status Icon Counting Rules:
   - âœ… RUNNING/STARTED = Count as "Running"
   - ğŸ’¤ IDLE = Count as "Idle" 
   - âš ï¸ APPLYING/ISSUES = Count as "Issues"
   - âŒ STOPPED/UNDEPLOYED = Count as "Stopped"
   - ğŸš¨ FAILED/CRITICAL = Count as "Critical"

3. Cross-validate with "ğŸš¦ Application Status Distribution" table:

VALIDATION PROCESS:
For each organization in hierarchy:
  hierarchy_counts = count_by_status_from_hierarchy(org_name)
  table_counts = get_status_counts_from_table(org_name)
  
  For each status (Running, Idle, Issues, Stopped, Critical):
    If hierarchy_counts[status] != table_counts[status]:
      â†’ FLAG MISMATCH ERROR
      â†’ Log: "ORG: {org} - {status}: Hierarchy={hierarchy_count}, Table={table_count}"
      â†’ STOP REPORT GENERATION
      â†’ FIX table counts to match hierarchy
      â†’ RE-RUN validation

EXAMPLE VALIDATION:
âœ… CORRECT: 
   Hierarchy shows: ANA has 10 apps with âœ… status
   Table shows: ANA "Running" = 10
   
âŒ INCORRECT:
   Hierarchy shows: B2B Partner Manager has 11 apps with âœ… status  
   Table shows: B2B Partner Manager "Running" = 14
   â†’ MUST FIX table to match hierarchy count

4. Validate TOTALS row matches sum of all organization counts

CRITICAL SUCCESS CRITERIA:
âœ… Every organization's table counts EXACTLY match hierarchy counts
âœ… TOTALS row equals sum of all individual organization counts  
âœ… No discrepancies between hierarchy display and status distribution table
âœ… Applications from platform insights but not in standard environments properly counted (e.g., kiet.yap org)

This validation prevents counting inconsistencies and ensures the status distribution table is 100% accurate based on the actual deployed hierarchy.
```

##### 4.6.2: Application Count Cross-Validation
```
AUTOMATED COUNT VERIFICATION:

1. **Count applications in hierarchy**: Count all "â”œâ”€â”€â”€ [app-name]" entries in report
2. **Count applications from data**: Sum all applications from list_applications calls
3. **Cross-validate**: hierarchy_count MUST equal data_collection_count

Validation Rules:
- If hierarchy_count < data_collection_count: MISSING APPLICATIONS
- If hierarchy_count > data_collection_count: DUPLICATE APPLICATIONS
- If hierarchy_count = data_collection_count: âœ… PASSED

Required Action if validation fails:
â†’ IDENTIFY missing/duplicate applications by name
â†’ FIX hierarchy section
â†’ RE-RUN validation until counts match exactly
```

##### 4.6.3: Detail Structure Completeness Audit
```
AUTOMATED DETAIL AUDIT:

For each application found in hierarchy:
  Check presence of required detail structure:
  
  Required Structure (EXACT format):
  â”œâ”€â”€â”€ [app-name] [STATUS]
  â”‚    â”œâ”€â”€â”€ ğŸ’¾ [Platform]: [vCores] vCore Ã— [replicas] replica/worker
  â”‚    â”œâ”€â”€â”€ ğŸ“ˆ Flows: [X] | Messages: [X] | Data: [X] GB throughput  
  â”‚    â”œâ”€â”€â”€ ğŸ‘¥ Consumers: [X] | ğŸ›¡ï¸ API Policies: [X] ([Details])
  â”‚    â””â”€â”€â”€ ğŸ•’ Last Updated: [Date]

  Validation Checks:
  - [x] Application name line present
  - [x] ğŸ’¾ Platform/resource line present
  - [x] ğŸ“ˆ Metrics line present  
  - [x] ğŸ‘¥ Consumer/policy line present
  - [x] ğŸ•’ Update date line present
  
  If ANY check fails:
    â†’ Application flagged as "INCOMPLETE STRUCTURE"
    â†’ STOP report generation
    â†’ FIX missing detail lines
    â†’ RE-RUN audit until 100% pass rate
```

##### 4.6.4: Error-Proof Workflow Execution
```
MANDATORY EXECUTION SEQUENCE:

Step 4.2: Generate hierarchy with application detail structure
   â†“
Step 4.6: Run automated validation (THIS STEP)
   â†“
IF validation PASSES:
   â†’ Proceed to Step 4.4: Save Final Report
   â†’ Mark as âœ… VALIDATED AND COMPLETE
   
IF validation FAILS:
   â†’ DO NOT SAVE REPORT
   â†’ FIX identified issues
   â†’ RE-RUN Step 4.6 validation
   â†’ Only save when validation passes

VALIDATION SUCCESS CRITERIA:
âœ… Application count matches exactly (hierarchy = data collection)
âœ… Zero applications with incomplete detail structure  
âœ… All required icons present (ğŸ’¾, ğŸ“ˆ, ğŸ‘¥, ğŸ•’)
âœ… No applications listed with just status line
```

---

## ğŸ”„ **REFRESH WORKFLOW**

To regenerate reports with current data using the Enhanced Dynamic Discovery approach:

### **ğŸ”„ Complete Dynamic Workflow Execution**

```bash
# Step 1: Platform Insights Discovery
get_platform_insights(includeFlows=true, includeMessages=true, includeDataThroughput=true)

# Step 2: Dynamic Business Group/Environment Extraction  
# Parse insights response to build organization/environment matrix:
discovered_orgs = extract_unique_organizations(insights.response)
discovered_envs = extract_unique_environments_per_org(insights.response)

# Example extracted inventory:
# org-123 ("Acme Corp") â†’ ["Production", "Sandbox"]
# org-456 ("Beta Ltd") â†’ ["Prod", "Dev", "Staging"]  
# org-789 ("Gamma Inc") â†’ ["Live", "Test"]

# Step 3: Dynamic Environment Validation Loop
validated_environments = []
For each (org_id, env_name) in discovered_envs:
  validation_result = list_applications(environmentName=env_name, organizationId=org_id, page=1)
  If validation_result.success:
    validated_environments.append({org_id, org_name, env_name})
  Else:
    log_inaccessible_environment(org_id, env_name, validation_result.error)

# Step 4: Complete Application Collection with Dynamic Pagination  
all_applications = {}
For each validated_env in validated_environments:
  page = 1
  While True:
    apps_response = list_applications(
      environmentName=validated_env.env_name, 
      organizationId=validated_env.org_id, 
      page=page
    )
    
    all_applications[org_id][env_name].extend(apps_response.applications)
    
    If page >= apps_response.total_pages:
      Break
    page += 1

# Step 5: Generate Timestamped Report
load_template("anypoint-assessment-template.md")
populate_template_with_dynamic_data(all_applications, validated_environments)
save_report(f"anypoint-platform-assessment-report-{current_timestamp}.md")
```

---

## ğŸ¯ **Key Dynamic Advantages**

**âœ… No Static Environment Names**: Discovers whatever environments actually exist  
**âœ… Organization Agnostic**: Works with any number of business groups  
**âœ… Auto-Pagination**: Handles environments with 100+ applications automatically  
**âœ… Access Control Aware**: Gracefully handles environments with restricted access  
**âœ… Future-Proof**: Automatically discovers new environments and organizations  

### **ğŸ”§ Implementation Notes**

1. **Discovery Phase**: Extract (org_id, org_name, env_id, env_name) tuples from platform insights
2. **Validation Phase**: Test each discovered environment with actual API calls  
3. **Collection Phase**: Use dynamic pagination to collect ALL applications per environment
4. **Error Handling**: Log and skip inaccessible environments without breaking the workflow
5. **Reporting Phase**: Generate comprehensive report using template with actual discovered data

This approach eliminates static assumptions and provides complete visibility into the actual Anypoint Platform deployment regardless of naming conventions or organizational structure.

---

## âš ï¸ **WORKFLOW DATA QUALITY CONSIDERATIONS**

### **Platform Insights Limitations:**
- **Missing env_type Field:** Platform insights may not populate environment type consistently
- **Inconsistent Classification:** Environment classification may be unreliable
- **All Environments Included:** Platform insights aggregates ALL environments (production + sandbox)

### **Environment Validation Requirements:**
1. **Never assume environment type** from platform insights alone
2. **Always test multiple environment name variations** per organization
3. **Cross-reference** environment IDs from insights with successful API calls
4. **User validation required** when environment classification is unclear
5. **Document validated environment names** for future reference

### **Execution Best Practices:**
- **Sequential Execution**: Execute workflow steps in order for proper data dependencies
- **Error Handling**: Continue processing even if some environments are inaccessible
- **Pagination Handling**: Always check for multiple pages in large environments
- **Data Validation**: Cross-reference platform insights with actual application data
- **Template Preservation**: Keep template file unchanged during report generation

---

**Workflow Version:** 1.0  
**Created:** February 12, 2026  
**Purpose:** Separate workflow execution logic from report template  
**Template File:** `anypoint-assessment-template.md`
**Output Format:** `anypoint-platform-assessment-report-YYYYMMDD-HHMMSS.md`
